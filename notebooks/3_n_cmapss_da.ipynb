{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89q7mbPv9NjU"
      },
      "source": [
        "# Part 3: Prognostics for Turbofan Engines ü©∫\n",
        "\n",
        "Prognostics is the prediction of Remaining Useful Life (RUL) until failure of a component based on the knowledge about current and future operation conditions (obtained through various sensors or physical models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFd6spkDEvi4"
      },
      "source": [
        "## 0. Introduction\n",
        "\n",
        "The new C-MAPSS dataset DS02 from NASA provides degradation trajectories of 9 turbofan engines with unknown and different initial health condition for complete flights and two failure modes (HPT efficiency degradation & HPT efficiency degradation combined with LPT efficiency and capacity degradation). The data were synthetically generated with the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dynamical model. The data contains multivariate sensors readings of the complete run-to-failure trajectories. Therefore, the records stop at the cycle/time the engine failed. A total number of 6.5M time stamps are available. Dataset copyright (c) by Manuel Arias.\n",
        "\n",
        "**For training simplicity, the dataset has been preprocessed. The dataset has been downsampled from 1Hz to 0.1 Hz with an IIR 8th Order Chebyshev filter. Data format has been converted from double to float precison.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CVNWlBJ9Ija"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import time \n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "from torch.utils.data import DataLoader, Dataset, SubsetRandomSampler\n",
        "from tqdm import tqdm\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ccLNkIa9Xvx",
        "outputId": "1dd1e3d4-4587-46f1-871e-f72159be57c3"
      },
      "outputs": [],
      "source": [
        "folder = os.getcwd()\n",
        "filename = \"../datasets/N-CMAPSS_DS02.csv\"\n",
        "print(filename)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3y1SgKPqSwq"
      },
      "source": [
        "## 1. Data exploration üîç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Au1CJA3b95-f",
        "outputId": "b873d351-3e30-4240-d04e-c24c4ffc9520"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(filename)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "0pxG0gHm_Jc9",
        "outputId": "0cdca513-b6e5-403c-91a0-8c99c83d8159"
      },
      "outputs": [],
      "source": [
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVxR_2YcsAun"
      },
      "source": [
        "## 2. Feature Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-WToY0EQ8gJ"
      },
      "outputs": [],
      "source": [
        "LABELS = [\"RUL\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQUvTSv8GSeE"
      },
      "source": [
        "Operating Conditions ($w$)\n",
        "\n",
        "DASHlink- Flight Data For Tail 687.(2012). Retrieved on 2019-01-29 from https://c3.nasa.gov/dashlink/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrHh3rYqGhGt"
      },
      "outputs": [],
      "source": [
        "W_VAR = [\"alt\", \"Mach\", \"TRA\", \"T2\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQvRwIJqGN7H"
      },
      "source": [
        "Sensor readings ($X_s$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjgyPJPNGiOI"
      },
      "outputs": [],
      "source": [
        "XS_VAR = [\n",
        "    \"T24\",\n",
        "    \"T30\",\n",
        "    \"T48\",\n",
        "    \"T50\",\n",
        "    \"P15\",\n",
        "    \"P2\",\n",
        "    \"P21\",\n",
        "    \"P24\",\n",
        "    \"Ps30\",\n",
        "    \"P40\",\n",
        "    \"P50\",\n",
        "    \"Nf\",\n",
        "    \"Nc\",\n",
        "    \"Wf\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFJVdOWqqekG"
      },
      "source": [
        "## 3. Developing your first prognostics model üíª\n",
        "Step by step, you will learn how to build your first prognostics model from scratch!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNBXLCmgFImE"
      },
      "source": [
        "### 3.1 Define Sequence Dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVFFWkhwFHP3"
      },
      "outputs": [],
      "source": [
        "class SlidingWindowDataset(Dataset):\n",
        "    def __init__(self, dataframe, window=50, stride=1, horizon=1, device=\"cpu\"):\n",
        "        \"\"\"Sliding window dataset with RUL label\n",
        "\n",
        "        Args:\n",
        "            dataframe (pd.DataFrame): dataframe containing scenario descriptors and sensor reading\n",
        "            window (int, optional): sequence window length. Defaults to 50.\n",
        "            stride (int, optional): data stride length. Defaults to 1.\n",
        "            horizon (int, optional): prediction forcasting length. Defaults to 1.\n",
        "        \"\"\"\n",
        "        self.window = window\n",
        "        self.stride = stride\n",
        "        self.horizon = horizon\n",
        "\n",
        "        self.X = np.array(dataframe[XS_VAR + W_VAR].values).astype(np.float32)\n",
        "        self.y = np.array(dataframe[\"RUL\"].values).astype(np.float32)\n",
        "        if \"ds\" in dataframe.columns:\n",
        "            unqiue_cycles = dataframe[[\"ds\", \"unit\", \"cycle\"]].value_counts(sort=False)\n",
        "        else:\n",
        "            unqiue_cycles = dataframe[[\"unit\", \"cycle\"]].value_counts(sort=False)\n",
        "        self.indices = torch.from_numpy(self._get_indices(unqiue_cycles)).to(device)\n",
        "\n",
        "    # TODO add comment\n",
        "    def _get_indices(self, unqiue_cycles):\n",
        "        cycles = unqiue_cycles.to_numpy()\n",
        "        idx_list = []\n",
        "        for i, c_count in enumerate(cycles):\n",
        "            c_start = sum(cycles[:i])\n",
        "            c_end = c_start + (c_count - self.window - self.horizon)\n",
        "            if c_end + self.horizon < len(\n",
        "                self.X\n",
        "            ):  # handling y not in the last seq case\n",
        "                idx_list += [_ for _ in np.arange(c_start, c_end + 1, self.stride)]\n",
        "        return np.asarray([(idx, idx + self.window) for idx in idx_list])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        i_start, i_stop = self.indices[i]\n",
        "        x = self.X[i_start:i_stop, :]\n",
        "        y = self.y[i_start]\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8_R6aEIG2Iy"
      },
      "outputs": [],
      "source": [
        "def create_datasets(df, window_size, train_units, test_units, device=\"cpu\"):\n",
        "    df_train = df[df[\"unit\"].isin(train_units)]\n",
        "    train_dataset = SlidingWindowDataset(df_train, window=window_size)\n",
        "\n",
        "    df_test = df[df[\"unit\"].isin(test_units)]\n",
        "    test_dataset = SlidingWindowDataset(df_test, window=window_size)\n",
        "\n",
        "    # normalizing features\n",
        "    scaler = MinMaxScaler()\n",
        "    train_dataset.X = scaler.fit_transform(train_dataset.X)\n",
        "    test_dataset.X = scaler.transform(test_dataset.X)\n",
        "\n",
        "    # convert numpy array to tensors\n",
        "    datasets = [train_dataset, test_dataset]\n",
        "    for d in datasets:\n",
        "        d.X = torch.from_numpy(d.X).to(device)\n",
        "        d.y = torch.from_numpy(d.y).to(device)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "def create_data_loaders(datasets, batch_size=256, val_split=0.2):\n",
        "    d_train, d_test = datasets\n",
        "    dataset_size = len(d_train)\n",
        "    indices = list(range(dataset_size))\n",
        "    split = int(np.floor(val_split * dataset_size))\n",
        "    np.random.shuffle(indices)\n",
        "    train_indices, val_indices = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "    train_loader = DataLoader(d_train, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(d_train, batch_size=batch_size, sampler=valid_sampler)\n",
        "    test_loader = DataLoader(d_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    d_info = f\"train_size: {len(train_indices)}\\t\"\n",
        "    d_info += f\"validation_size: {len(val_indices)}\\t\"\n",
        "    d_info += f\"test_size: {len(d_test)}\"\n",
        "    print(d_info)\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iuOuWQYuHlw"
      },
      "source": [
        "### 3.2 Define Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQ6S9PhqOA_G"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        optimizer,\n",
        "        n_epochs=20,\n",
        "        criterion=nn.MSELoss(),\n",
        "        model_name=\"best_model\",\n",
        "        device=\"cpu\",\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.n_epochs = n_epochs\n",
        "        self.criterion = criterion\n",
        "\n",
        "        # adding time_stamp to model name to make sure the save models don't overwrite each other,\n",
        "        # you can customize your own model name with hyperparameters so that you can reload the model more easily\n",
        "        time_stamp = time.strftime(\"%m%d%H%M%S\")\n",
        "        self.model_path = f\"models/{model_name}_{time_stamp}.pt\"\n",
        "\n",
        "        self.losses = {split: [] for split in [\"train\", \"eval\", \"test\"]}\n",
        "\n",
        "    def compute_loss(self, x, y, model=None):\n",
        "        y = y.view(-1)\n",
        "        y_pred = self.model(x)\n",
        "        y_pred = y_pred.view(-1)\n",
        "        loss = self.criterion(y, y_pred)\n",
        "        return loss, y_pred, y\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        self.model.train()\n",
        "        # batch losses\n",
        "        b_losses = []\n",
        "        for x, y in loader:\n",
        "            # Setting the optimizer gradient to Zero\n",
        "            self.optimizer.zero_grad()\n",
        "            x.to(torch.device(self.device))\n",
        "            y.to(torch.device(self.device))\n",
        "\n",
        "            loss, pred, target = self.compute_loss(x, y)\n",
        "\n",
        "            # Backpropagate the training loss\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            b_losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        # aggregated losses across batches\n",
        "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
        "        self.losses[\"train\"].append(agg_loss)\n",
        "        return agg_loss\n",
        "\n",
        "    # decorator, equivalent to with torch.no_grad():\n",
        "    @torch.no_grad()\n",
        "    def eval_epoch(self, loader, split=\"eval\"):\n",
        "        self.model.eval()\n",
        "        # batch losses\n",
        "        b_losses = []\n",
        "        for x, y in loader:\n",
        "            x.to(torch.device(self.device))\n",
        "            y.to(torch.device(self.device))\n",
        "\n",
        "            loss, pred, target = self.compute_loss(x, y)\n",
        "\n",
        "            b_losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "        # aggregated losses across batches\n",
        "        agg_loss = np.sqrt((np.asarray(b_losses) ** 2).mean())\n",
        "        self.losses[split].append(agg_loss)\n",
        "        return agg_loss\n",
        "\n",
        "    def fit(self, loaders):\n",
        "        print(f\"Training model for {self.n_epochs} epochs...\")\n",
        "        train_loader, eval_loader, test_loader = loaders\n",
        "        train_start = time.time()\n",
        "\n",
        "        start_epoch = 0\n",
        "        best_eval_loss = np.inf\n",
        "\n",
        "        for epoch in range(start_epoch, self.n_epochs):\n",
        "            epoch_start = time.time()\n",
        "\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            eval_loss = self.eval_epoch(eval_loader, split=\"eval\")\n",
        "            test_loss = self.eval_epoch(test_loader, split=\"test\")\n",
        "\n",
        "            if eval_loss < best_eval_loss:\n",
        "                best_eval_loss = eval_loss\n",
        "                self.save(self.model, self.model_path)\n",
        "\n",
        "            s = (\n",
        "                f\"[Epoch {epoch + 1}] \"\n",
        "                f\"train_loss = {train_loss:.5f}, \"\n",
        "                f\"eval_loss = {eval_loss:.5f}, \"\n",
        "                f\"test_loss = {test_loss:.5f}\"\n",
        "            )\n",
        "\n",
        "            epoch_time = time.time() - epoch_start\n",
        "            s += f\" [{epoch_time:.1f}s]\"\n",
        "            print(s)\n",
        "\n",
        "        train_time = int(time.time() - train_start)\n",
        "\n",
        "        print(f\"Task done in {train_time}s\")\n",
        "\n",
        "    # decorator, equivalent to with torch.no_grad():\n",
        "    @torch.no_grad()\n",
        "    def eval_rul_prediction(self, test_loader):\n",
        "        print(f\"Evaluating test RUL...\")\n",
        "\n",
        "        ## MASK OUT EVAL and add explanation\n",
        "        best_model = self.load(self.model)\n",
        "        best_model.eval()\n",
        "\n",
        "        preds = []\n",
        "        trues = []\n",
        "\n",
        "        for x, y in tqdm(test_loader):\n",
        "            x = x.to(self.device)\n",
        "            y = y.to(self.device)\n",
        "\n",
        "            _, y_pred, y_target = self.compute_loss(x, y)\n",
        "            preds.append(y_pred.detach().cpu().numpy())\n",
        "            trues.append(y_target.detach().cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(preds, axis=0)\n",
        "        trues = np.concatenate(trues, axis=0)\n",
        "\n",
        "        df = pd.DataFrame(\n",
        "            {\"pred\": preds, \"true\": trues, \"err\": np.sqrt((preds - trues) ** 2)}\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def save(self, model, model_path=None):\n",
        "        os.makedirs(f\"{folder}/models\", exist_ok=True)\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    def load(self, model, model_path=None):\n",
        "        \"\"\"\n",
        "        loads the prediction model's parameters\n",
        "        \"\"\"\n",
        "        if model_path is None:\n",
        "            model_path = self.model_path\n",
        "        model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
        "        print(\n",
        "            f\"Model {model.__class__.__name__} saved in {model_path} loaded to {self.device}\"\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def plot_losses(self):\n",
        "        \"\"\"\n",
        "        :param losses: dict with losses\n",
        "        \"\"\"\n",
        "        linestyles = {\n",
        "            \"train\": \"solid\",\n",
        "            \"eval\": \"dashed\",\n",
        "            \"test\": \"dotted\",\n",
        "        }\n",
        "        for split, loss in self.losses.items():\n",
        "            ls = linestyles[split]\n",
        "            plt.plot(range(1, 1 + len(loss)), loss, label=f\"{split} loss\", linestyle=ls)\n",
        "            plt.yscale(\"log\")\n",
        "\n",
        "        plt.title(\"Training/Validation Losses\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igknd0rYN56n"
      },
      "source": [
        "### 3.3 Define Model: 1DCNN\n",
        "\n",
        "Conventional CNN's developed for image tasks learn to extract features from the 2D input data. They are autonomous (require no domain expertise or prior info about the image) and thus can be applied to any image regardless of its dimensions. This is due to the fact that these CNN's go through an image by downsampling the image which we call straddling or windowing.  \n",
        "\n",
        "Similarly 1D CNN learns to extract features from a time series data, by windowing over the data, considering a set of data observations each time. The benefit of using the CNN for sequence classification is that it can learn from the raw time series data, and in turn do not require domain expertise to engineer relevant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K831VUjPLSlC"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    if isinstance(m, nn.BatchNorm1d):\n",
        "        m.weight.data.fill_(1.0)\n",
        "        m.bias.data.zero_()\n",
        "    elif isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
        "        m.weight.data = nn.init.xavier_uniform_(\n",
        "            m.weight.data, gain=nn.init.calculate_gain(\"relu\")\n",
        "        )\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def Single1DCNNLayer(\n",
        "    in_channels, out_channels=1, dropout=0.2, kernel_size=10, padding=\"same\"\n",
        "):\n",
        "    # mask out batchnorm1d\n",
        "    layer = [\n",
        "        nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding),\n",
        "        nn.BatchNorm1d(out_channels),\n",
        "    ]\n",
        "    if dropout > 0.0:\n",
        "        layer.append(nn.Dropout(dropout))\n",
        "    layer.append(nn.ReLU())\n",
        "    return layer\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    A 1D-CNN model that is based on the paper \"Fusing physics-based and deep learning models for prognostics\"\n",
        "    from Manuel Arias Chao et al. (with batchnorm layers)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=18,\n",
        "        out_channels=1,\n",
        "        window=50,\n",
        "        n_ch=10,\n",
        "        n_k=10,\n",
        "        n_hidden=50,\n",
        "        n_layers=3,\n",
        "        dropout=0.0,\n",
        "        padding=\"same\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_features (int, optional): number of input features. Defaults to 18.\n",
        "            window (int, optional): sequence length. Defaults to 50.\n",
        "            n_ch (int, optional): number of channels (filter size). Defaults to 10.\n",
        "            n_k (int, optional): kernel size. Defaults to 10.\n",
        "            n_hidden (int, optional): number of hidden neurons for regressor. Defaults to 50.\n",
        "            n_layers (int, optional): number of convolution layers. Defaults to 5.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            dim_in = in_channels if i == 0 else n_ch\n",
        "            dim_out = n_ch if i != n_layers - 1 else 1\n",
        "            layers += Single1DCNNLayer(\n",
        "                dim_in, dim_out, dropout=dropout, kernel_size=n_k, padding=padding\n",
        "            )\n",
        "\n",
        "        self.cnn_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Regression Output\n",
        "        post_layers = [nn.Linear(in_features=window, out_features=n_hidden)]\n",
        "        post_layers += [\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=n_hidden, out_features=out_channels),\n",
        "        ]\n",
        "        self.rul_regressor = nn.Sequential(*post_layers)\n",
        "        # Recursive apply weight initialization for all models\n",
        "        self.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Change input shape from (b, window_size, in_channels) to (b, in_channels, window_size)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Propagate input through Conv-Layers\n",
        "        feature = self.cnn_layers(x)\n",
        "\n",
        "        # Flatten the Feature Map\n",
        "        feature = torch.flatten(feature, start_dim=1)\n",
        "        output = self.rul_regressor(feature)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-JGs7gqhn8w"
      },
      "source": [
        "### 3.4 Training a model instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84R7i9gF4KFD"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    r\"\"\"Sets the seed for generating random numbers in PyTorch, numpy and\n",
        "    Python.\n",
        "\n",
        "    Args:\n",
        "        seed (int): The desired seed.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ2ePKogyvCq"
      },
      "outputs": [],
      "source": [
        "# dataset parameters\n",
        "train_units = [16, 18, 20]\n",
        "test_units = [15]\n",
        "window_size = 50\n",
        "\n",
        "# training parameters\n",
        "batch_size = 256\n",
        "base_lr = 5e-3\n",
        "weight_decay = 1e-5\n",
        "max_epochs = 10\n",
        "\n",
        "# CNN model parameters\n",
        "in_channels = 18\n",
        "out_channels = 1\n",
        "n_ch = 10\n",
        "n_k = 9\n",
        "n_hidden = 50\n",
        "n_layers = 3\n",
        "dropout = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WqMyBWqyk9U",
        "outputId": "e3f3b51b-8a0b-4cbf-844d-1f4957d62e33"
      },
      "outputs": [],
      "source": [
        "SEED = 271828  # choose your magic number\n",
        "seed_everything(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = create_datasets(\n",
        "    df, window_size=50, train_units=train_units, test_units=test_units, device=device\n",
        ")\n",
        "loaders = create_data_loaders(datasets, batch_size=batch_size)\n",
        "\n",
        "model = CNN(\n",
        "    in_channels=in_channels,\n",
        "    out_channels=out_channels,\n",
        "    n_ch=n_ch,\n",
        "    n_k=n_k,\n",
        "    n_hidden=n_hidden,\n",
        "    n_layers=n_layers,\n",
        "    dropout=dropout,\n",
        ").to(device)\n",
        "print(summary(model))\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=base_lr, weight_decay=weight_decay)\n",
        "\n",
        "criterion = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "y23Vz1uusZfY",
        "outputId": "32d12cd8-bff0-45b9-d3b3-c1d76aec4be0"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model, optimizer, criterion=criterion, n_epochs=max_epochs, device=device\n",
        ")\n",
        "\n",
        "trainer.fit(loaders)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "IkwLJMN-sADH",
        "outputId": "1a94ff11-4367-42d6-8263-b4c04200d2d5"
      },
      "outputs": [],
      "source": [
        "trainer.plot_losses()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.save(model.state_dict(), \"./models/cmapss_baseline.pt\")\n",
        "# model.load_state_dict(torch.load(\"./models/cmapss_baseline.pt\", map_location=device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "fPONTMrK7t2P",
        "outputId": "a0a9ebbb-f5b8-4b27-a91d-150f837d6595"
      },
      "outputs": [],
      "source": [
        "df_test = trainer.eval_rul_prediction(loaders[2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "SXyGipjP77TV",
        "outputId": "5298e72a-ab4c-4859-c291-b1f084f4e7ec"
      },
      "outputs": [],
      "source": [
        "df_test.plot(y=[\"true\", \"pred\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBvvfE7VUIRj"
      },
      "source": [
        "### 3.6 Introduce NASA score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1QerdrDUCnj"
      },
      "outputs": [],
      "source": [
        "def nasa_score(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Nasa score (in 1E5) as introduced in https://arxiv.org/abs/2003.00732\n",
        "    \"\"\"\n",
        "    d = y_pred - y_true\n",
        "    score = np.sum(np.exp(d[d >= 0] / 10) - 1) + np.sum(np.exp(-d[d < 0] / 13) - 1)\n",
        "    return score / 1e5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sDul_z5WVvr",
        "outputId": "8b3b1b44-5111-4cba-eefd-dab9b8d2aed0"
      },
      "outputs": [],
      "source": [
        "nasa_score(df_test[\"pred\"], df_test[\"true\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmse = np.sqrt(np.mean((df_test[\"pred\"] - df_test[\"true\"]) ** 2))\n",
        "print(rmse)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "imc2025",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
